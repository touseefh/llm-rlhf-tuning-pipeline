{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages if running locally or in a new environment\n",
    "# !pip3 install google-cloud-pipeline-components\n",
    "# !pip3 install kfp\n",
    "# !pip3 install google-cloud-aiplatform\n",
    "# !pip install tensorboard\n",
    "\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from kfp import compiler\n",
    "from google_cloud_pipeline_components.preview.llm import rlhf_pipeline\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from utils import authenticate, print_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compile-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to the yaml file\n",
    "RLHF_PIPELINE_PKG_PATH = \"rlhf_pipeline.yaml\"\n",
    "\n",
    "# Execute the compile function\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=rlhf_pipeline,\n",
    "    package_path=RLHF_PIPELINE_PKG_PATH\n",
    ")\n",
    "\n",
    "# Verify the file was created\n",
    "!head rlhf_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc-reward-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Reward Model Training Steps ---\n",
    "\n",
    "# Preference dataset size and batch size\n",
    "PREF_DATASET_SIZE = 3000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "REWARD_STEPS_PER_EPOCH = math.ceil(PREF_DATASET_SIZE / BATCH_SIZE)\n",
    "REWARD_NUM_EPOCHS = 30\n",
    "\n",
    "# Calculate number of steps in the reward model training\n",
    "reward_model_train_steps = REWARD_STEPS_PER_EPOCH * REWARD_NUM_EPOCHS\n",
    "print(f\"Reward Model Train Steps: {reward_model_train_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc-rl-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Reinforcement Learning Training Steps ---\n",
    "\n",
    "# Prompt dataset size\n",
    "PROMPT_DATASET_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "RL_STEPS_PER_EPOCH = math.ceil(PROMPT_DATASET_SIZE / BATCH_SIZE)\n",
    "RL_NUM_EPOCHS = 10\n",
    "\n",
    "# Calculate the number of steps in the RL training\n",
    "reinforcement_learning_train_steps = RL_STEPS_PER_EPOCH * RL_NUM_EPOCHS\n",
    "print(f\"Reinforcement Learning Train Steps: {reinforcement_learning_train_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter values for the pipeline\n",
    "parameter_values={\n",
    "        \"preference_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n",
    "        \"prompt_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl\",\n",
    "        \"eval_dataset\": \\\n",
    "    \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl\",\n",
    "        \"large_model_reference\": \"llama-2-7b\",\n",
    "        \"reward_model_train_steps\": reward_model_train_steps,\n",
    "        \"reinforcement_learning_train_steps\": reinforcement_learning_train_steps,\n",
    "        \"reward_model_learning_rate_multiplier\": 1.0,\n",
    "        \"reinforcement_learning_rate_multiplier\": 1.0,\n",
    "        \"kl_coeff\": 0.1, # increased to reduce reward hacking\n",
    "        \"instruction\":\\\n",
    "    \"Summarize in less than 50 words\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auth-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and Initialize Vertex AI\n",
    "credentials, PROJECT_ID, STAGING_BUCKET = authenticate()\n",
    "REGION = \"europe-west4\"\n",
    "\n",
    "aiplatform.init(project = PROJECT_ID,\n",
    "                location = REGION,\n",
    "                credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-job",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"tutorial-rlhf-tuning\",\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    template_path=RLHF_PIPELINE_PKG_PATH,\n",
    "    parameter_values=parameter_values)\n",
    "\n",
    "# Run the pipeline job\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-section-header",
   "metadata": {},
   "source": [
    "--- \n",
    "### Model Evaluation\n",
    "The following cells handle Tensorboard monitoring and comparing the tuned vs untuned model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard-logs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tensorboard for reward logs\n",
    "port = %env PORT1\n",
    "%tensorboard --logdir reward-logs --port $port --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard-reinforcer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tensorboard for reinforcer logs\n",
    "port = %env PORT2\n",
    "%tensorboard --logdir reinforcer-logs --port $port --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard-full",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Tensorboard for full data logs\n",
    "port = %env PORT3\n",
    "%tensorboard --logdir reinforcer-fulldata-logs --port $port --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-eval-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Evaluation Data (Tuned vs Untuned)\n",
    "eval_tuned_path = 'eval_results_tuned.jsonl'\n",
    "eval_untuned_path = 'eval_results_untuned.jsonl'\n",
    "\n",
    "eval_data_tuned = []\n",
    "eval_data_untuned = []\n",
    "\n",
    "# Read Tuned Data\n",
    "with open(eval_tuned_path) as f:\n",
    "    for line in f:\n",
    "        eval_data_tuned.append(json.loads(line))\n",
    "\n",
    "# Read Untuned Data\n",
    "with open(eval_untuned_path) as f:\n",
    "    for line in f:\n",
    "        eval_data_untuned.append(json.loads(line))\n",
    "\n",
    "# Print sample to verify\n",
    "from utils import print_d\n",
    "print(\"Tuned Sample:\")\n",
    "print_d(eval_data_tuned[0])\n",
    "print(\"\\nUntuned Sample:\")\n",
    "print_d(eval_data_untuned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "\n",
    "# Extract prompts\n",
    "prompts = [sample['inputs']['inputs_pretokenized'] for sample in eval_data_tuned]\n",
    "\n",
    "# Extract completions\n",
    "untuned_completions = [sample['prediction'] for sample in eval_data_untuned]\n",
    "tuned_completions = [sample['prediction'] for sample in eval_data_tuned]\n",
    "\n",
    "# Build DataFrame\n",
    "results = pd.DataFrame(\n",
    "    data={'prompt': prompts,\n",
    "          'base_model':untuned_completions,\n",
    "          'tuned_model': tuned_completions})\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}